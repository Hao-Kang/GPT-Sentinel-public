{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "In this notebook we will evaluate all three models we have up to now (RoBERTa-Sentinel, T5-Sentinel and GPT2-Detector as baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from Model import SentinelNonLinear, Sentinel, T5Sentinel\n",
    "from Dataset import OpenGPTDataset, GPT2_OutputDataset, download_gpt2\n",
    "from memoizer import memoize, load_cache\n",
    "\n",
    "SELF_NAME = \"evaluate_experiment.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_ROOT = Path(Path.cwd().parent.parent.parent)\n",
    "PATH_WEBTEXT = Path(PATH_ROOT, \"data\", \"open-web-text-split\")\n",
    "PATH_GPTTEXT = Path(PATH_ROOT, \"data\", \"open-gpt-text-split\")\n",
    "PATH_GPT2_OUTPUT = Path(PATH_ROOT, \"data\", \"gpt2-output\")\n",
    "\n",
    "PATH_CACHE = Path(PATH_ROOT, \"result\", \"cache\")\n",
    "\n",
    "PATH_CHECKPOINT_ROBERTA = Path(PATH_CACHE, \"roberta.base.0425.pt\")\n",
    "PATH_CHECKPOINT_T5 = Path(PATH_CACHE, \"t5.small.0422.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2 output dataset already downloaded.\n"
     ]
    }
   ],
   "source": [
    "if len([_ for _ in PATH_GPT2_OUTPUT.iterdir()]) == 0: download_gpt2(PATH_GPT2_OUTPUT)\n",
    "else: print(\"GPT2 output dataset already downloaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Human text: 100%|███████████████████████████████████| 5000/5000 [00:00<00:00, 100064.03it/s]\n",
      "Loading GPT text: 100%|██████████████████████████████████████| 5000/5000 [00:00<00:00, 90077.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<All data loaded>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "clean_opengpt_test = OpenGPTDataset(\n",
    "    str(Path(PATH_WEBTEXT, \"test.jsonl\")),\n",
    "    str(Path(PATH_GPTTEXT, \"test.jsonl\")),\n",
    "    force_match=False\n",
    ")\n",
    "\n",
    "dirty_opengpt_test = OpenGPTDataset(\n",
    "    str(Path(PATH_WEBTEXT, \"test-dirty.jsonl\")),\n",
    "    str(Path(PATH_GPTTEXT, \"test-dirty.jsonl\")),\n",
    "    force_match=False\n",
    ")\n",
    "\n",
    "gpt2xl_output_test = GPT2_OutputDataset(\n",
    "    Path(PATH_GPT2_OUTPUT, \"xl-1542M.test.jsonl\"),\n",
    "    Path(PATH_GPT2_OUTPUT, \"webtext.test.jsonl\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_statistics(prediction, threshold=0.5):\n",
    "    TP, TN, FP, FN = 0, 0, 0, 0\n",
    "    key:str\n",
    "    for key in prediction:\n",
    "        pred = prediction[key]\n",
    "        p_gpt, p_web = pred[0], pred[1]\n",
    "        pred_gpt = p_gpt > threshold\n",
    "        real_gpt = key.endswith(\"gpt\")\n",
    "\n",
    "        if pred_gpt and real_gpt: TP += 1\n",
    "        elif (not pred_gpt) and (not real_gpt): TN += 1\n",
    "        elif pred_gpt and (not real_gpt): FP += 1\n",
    "        else: FN += 1\n",
    "    \n",
    "    return TP, TN, FP, FN\n",
    "\n",
    "def report_statistics(TP, TN, FP, FN):\n",
    "    TPR = TP / (TP + FN)\n",
    "    TNR = TN / (TN + FP)\n",
    "    FPR = FP / (FP + TN)\n",
    "    FNR = FN / (FN + TP)\n",
    "    print(f\"True Positive: {TP} \\t| True Negative: {TN}\")\n",
    "    print(f\"False Positive:{FP} \\t| False Negative:{FN}\")\n",
    "    print(f\"True Positive Rate:  {round(TPR * 100, 2)}\\%\")\n",
    "    print(f\"True Negative Rate:  {round(TNR * 100, 2)}\\%\")\n",
    "    print(f\"False Positive Rate: {round(FPR * 100, 2)}\\%\")\n",
    "    print(f\"False Negative Rate: {round(FNR * 100, 2)}\\%\")\n",
    "    print(f\"Accuracy: {round(((TP + TN) / (TP + TN + FP + FN)) * 100, 2)}\\%\")\n",
    "    print(f\"F1 Score: {round((TP) / (TP + 0.5 * (FP + FN)), 2)}\")\n",
    "\n",
    "    print(\"LaTeX Usable-version\\n\")\n",
    "\n",
    "    print(\n",
    "    f\"{round(((TP + TN) / (TP + TN + FP + FN)) * 100, 2)}\\%\", \"&\"\n",
    "    f\"{round(TPR * 100, 2)}\\%, ({TP})\", \"&\",\n",
    "    f\"{round(TNR * 100, 2)}\\%, ({TN})\", \"&\",\n",
    "    f\"{round(FPR * 100, 2)}\\%, ({FP})\", \"&\",\n",
    "    f\"{round(FNR * 100, 2)}\\%, ({FN})\", \"\\\\\\\\\"\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate T5-Sentinel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Finish\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load(PATH_CHECKPOINT_T5)\n",
    "t5_model = T5Sentinel()\n",
    "t5_model.load_state_dict(checkpoint[\"model\"])\n",
    "t5_model = t5_model.to(\"cuda\")\n",
    "t5_model.eval()\n",
    "print(\"Load Finish\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T5-Sentinel on OpenGPTText-Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reusing existing cache from d:\\Projects\\GPT-Sentinel\\result\\cache\\eval_t5_opengpt_final.pt\n",
      "Cache is generated by evaluate_experiment.ipynb\n"
     ]
    }
   ],
   "source": [
    "@memoize(Path(PATH_CACHE, \"eval_t5_opengpt_final.pt\"), SELF_NAME)\n",
    "def calculate_t5_final():\n",
    "    t5_prediction_clean = dict()\n",
    "    for index in tqdm(range(len(clean_opengpt_test))):\n",
    "        (text, label), (uid, _) = clean_opengpt_test[index]\n",
    "        prediction, _ = t5_model(text)\n",
    "        uid = uid + (\"web\" if label == 0 else \"gpt\")\n",
    "        t5_prediction_clean[uid] = prediction.cpu().numpy()\n",
    "    return t5_prediction_clean\n",
    "\n",
    "t5_prediction_clean = calculate_t5_final()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5 on OpenGPTText-Final\n",
      "True Positive: 2906 \t| True Negative: 2863\n",
      "False Positive:81 \t| False Negative:38\n",
      "True Positive Rate:  98.71\\%\n",
      "True Negative Rate:  97.25\\%\n",
      "False Positive Rate: 2.75\\%\n",
      "False Negative Rate: 1.29\\%\n",
      "Accuracy: 97.98\\%\n",
      "F1 Score: 0.98\n",
      "LaTeX Usable-version\n",
      "\n",
      "97.98\\% &98.71\\%, (2906) & 97.25\\%, (2863) & 2.75\\%, (81) & 1.29\\%, (38) \\\\\n"
     ]
    }
   ],
   "source": [
    "t5_statistics = quick_statistics(t5_prediction_clean)\n",
    "print(\"T5 on OpenGPTText-Final\")\n",
    "report_statistics(*t5_statistics)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T5-Sentinel on OpenGPTText-Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reusing existing cache from d:\\Projects\\GPT-Sentinel\\result\\cache\\eval_t5_opengpt_original.pt\n",
      "Cache is generated by evaluate_experiment.ipynb\n"
     ]
    }
   ],
   "source": [
    "@memoize(Path(PATH_CACHE, \"eval_t5_opengpt_original.pt\"), SELF_NAME)\n",
    "def calculate_t5_original():\n",
    "    t5_prediction_orig = dict()\n",
    "    for index in tqdm(range(len(dirty_opengpt_test))):\n",
    "        (text, label), (uid, _) = dirty_opengpt_test[index]\n",
    "        prediction, _ = t5_model(text)\n",
    "        uid = uid + (\"web\" if label == 0 else \"gpt\")\n",
    "        t5_prediction_orig[uid] = prediction.cpu().numpy()\n",
    "    return t5_prediction_orig\n",
    "\n",
    "t5_prediction_orig = calculate_t5_original()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5 on OpenGPTText-Original\n",
      "True Positive: 2907 \t| True Negative: 2842\n",
      "False Positive:102 \t| False Negative:37\n",
      "True Positive Rate:  98.74\\%\n",
      "True Negative Rate:  96.54\\%\n",
      "False Positive Rate: 3.46\\%\n",
      "False Negative Rate: 1.26\\%\n",
      "Accuracy: 97.64\\%\n",
      "F1 Score: 0.98\n",
      "LaTeX Usable-version\n",
      "\n",
      "97.64\\% &98.74\\%, (2907) & 96.54\\%, (2842) & 3.46\\%, (102) & 1.26\\%, (37) \\\\\n"
     ]
    }
   ],
   "source": [
    "t5_statistics = quick_statistics(t5_prediction_orig)\n",
    "print(\"T5 on OpenGPTText-Original\")\n",
    "report_statistics(*t5_statistics)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T5-Sentinel on GPT2-Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reusing existing cache from d:\\Projects\\GPT-Sentinel\\result\\cache\\eval_t5_gpt2_output.pt\n",
      "Cache is generated by evaluate_experiment.ipynb\n"
     ]
    }
   ],
   "source": [
    "@memoize(Path(PATH_CACHE, \"eval_t5_gpt2_output.pt\"), SELF_NAME)\n",
    "def calculate_t5_gpt2():\n",
    "    t5_prediction_gpt2 = dict()\n",
    "    for index in tqdm(range(len(gpt2xl_output_test))):\n",
    "        text, label = gpt2xl_output_test[index]\n",
    "        prediction, _ = t5_model(text)\n",
    "        uid = str(index) + (\"web\" if label == 0 else \"gpt\")\n",
    "        t5_prediction_gpt2[uid] = prediction.cpu().numpy()\n",
    "    return t5_prediction_gpt2\n",
    "\n",
    "t5_prediction_gpt2 = calculate_t5_gpt2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5 on GPT2-Output\n",
      "True Positive: 165 \t| True Negative: 4703\n",
      "False Positive:297 \t| False Negative:4835\n",
      "True Positive Rate:  3.3\\%\n",
      "True Negative Rate:  94.06\\%\n",
      "False Positive Rate: 5.94\\%\n",
      "False Negative Rate: 96.7\\%\n",
      "Accuracy: 48.68\\%\n",
      "F1 Score: 0.06\n",
      "LaTeX Usable-version\n",
      "\n",
      "48.68\\% &3.3\\%, (165) & 94.06\\%, (4703) & 5.94\\%, (297) & 96.7\\%, (4835) \\\\\n"
     ]
    }
   ],
   "source": [
    "t5_statistics = quick_statistics(t5_prediction_gpt2)\n",
    "print(\"T5 on GPT2-Output\")\n",
    "report_statistics(*t5_statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "del t5_model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RoBERTa-Sentinel Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Finish\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load(PATH_CHECKPOINT_ROBERTA)\n",
    "roberta_model = SentinelNonLinear(\n",
    "    RobertaModel.from_pretrained(\"roberta-base\"),\n",
    "    RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    ")\n",
    "roberta_model.load_state_dict(checkpoint[\"model\"])\n",
    "roberta_model = roberta_model.to(\"cuda\")\n",
    "roberta_model.eval()\n",
    "print(\"Load Finish\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoBERTa on OpenGPTText-Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reusing existing cache from d:\\Projects\\GPT-Sentinel\\result\\cache\\eval_roberta_opengpt_final.pt\n",
      "Cache is generated by evaluate_experiment.ipynb\n"
     ]
    }
   ],
   "source": [
    "@memoize(Path(PATH_CACHE, \"eval_roberta_opengpt_final.pt\"), SELF_NAME)\n",
    "def calculate_roberta_final():\n",
    "    with torch.inference_mode():\n",
    "        roberta_prediction_clean = dict()\n",
    "        for index in tqdm(range(len(clean_opengpt_test))):\n",
    "            (text, label), (uid, _) = clean_opengpt_test[index]\n",
    "            prediction = torch.nn.functional.softmax(roberta_model(text).detach().squeeze(dim=0), dim=-1).cpu()\n",
    "            prediction = torch.Tensor([prediction[1], prediction[0]])\n",
    "            uid = uid + (\"web\" if label == 0 else \"gpt\")\n",
    "            roberta_prediction_clean[uid] = prediction.numpy()\n",
    "    return roberta_prediction_clean\n",
    "\n",
    "roberta_prediction_clean = calculate_roberta_final()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoBERTa on OpenGPTText-Final\n",
      "True Positive: 2850 \t| True Negative: 2680\n",
      "False Positive:264 \t| False Negative:94\n",
      "True Positive Rate:  96.81\\%\n",
      "True Negative Rate:  91.03\\%\n",
      "False Positive Rate: 8.97\\%\n",
      "False Negative Rate: 3.19\\%\n",
      "Accuracy: 93.92\\%\n",
      "F1 Score: 0.94\n",
      "LaTeX Usable-version\n",
      "\n",
      "93.92\\% &96.81\\%, (2850) & 91.03\\%, (2680) & 8.97\\%, (264) & 3.19\\%, (94) \\\\\n"
     ]
    }
   ],
   "source": [
    "roberta_statistics = quick_statistics(roberta_prediction_clean)\n",
    "print(\"RoBERTa on OpenGPTText-Final\")\n",
    "report_statistics(*roberta_statistics)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoBERTa on OpenGPTText-Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reusing existing cache from d:\\Projects\\GPT-Sentinel\\result\\cache\\eval_roberta_opengpt_original.pt\n",
      "Cache is generated by evaluate_experiment.ipynb\n"
     ]
    }
   ],
   "source": [
    "@memoize(Path(PATH_CACHE, \"eval_roberta_opengpt_original.pt\"), SELF_NAME)\n",
    "def calculate_roberta_original():\n",
    "    roberta_prediction_orig = dict()\n",
    "    for index in tqdm(range(len(dirty_opengpt_test))):\n",
    "        (text, label), (uid, _) = dirty_opengpt_test[index]\n",
    "        prediction = torch.nn.functional.softmax(roberta_model(text).detach().squeeze(dim=0), dim=-1).cpu()\n",
    "        prediction = torch.Tensor([prediction[1], prediction[0]])\n",
    "        uid = uid + (\"web\" if label == 0 else \"gpt\")\n",
    "        roberta_prediction_orig[uid] = prediction.numpy()\n",
    "    return roberta_prediction_orig\n",
    "\n",
    "roberta_prediction_orig = calculate_roberta_original()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoBERTa on OpenGPTText-Original\n",
      "True Positive: 2889 \t| True Negative: 2309\n",
      "False Positive:635 \t| False Negative:55\n",
      "True Positive Rate:  98.13\\%\n",
      "True Negative Rate:  78.43\\%\n",
      "False Positive Rate: 21.57\\%\n",
      "False Negative Rate: 1.87\\%\n",
      "Accuracy: 88.28\\%\n",
      "F1 Score: 0.89\n",
      "LaTeX Usable-version\n",
      "\n",
      "88.28\\% &98.13\\%, (2889) & 78.43\\%, (2309) & 21.57\\%, (635) & 1.87\\%, (55) \\\\\n"
     ]
    }
   ],
   "source": [
    "roberta_statistics = quick_statistics(roberta_prediction_orig)\n",
    "print(\"RoBERTa on OpenGPTText-Original\")\n",
    "report_statistics(*roberta_statistics)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoBERTa on GPT2-Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reusing existing cache from d:\\Projects\\GPT-Sentinel\\result\\cache\\eval_roberta_gpt2_output.pt\n",
      "Cache is generated by evaluate_experiment.ipynb\n"
     ]
    }
   ],
   "source": [
    "@memoize(Path(PATH_CACHE, \"eval_roberta_gpt2_output.pt\"), SELF_NAME)\n",
    "def calculate_roberta_gpt2():\n",
    "    roberta_prediction_gpt2 = dict()\n",
    "    for index in tqdm(range(len(gpt2xl_output_test))):\n",
    "        text, label = gpt2xl_output_test[index]\n",
    "        prediction = torch.nn.functional.softmax(roberta_model(text).detach().squeeze(dim=0), dim=-1).cpu()\n",
    "        prediction = torch.Tensor([prediction[1], prediction[0]])\n",
    "        uid = str(index) + (\"web\" if label == 0 else \"gpt\")\n",
    "        roberta_prediction_gpt2[uid] = prediction.cpu().numpy()\n",
    "    return roberta_prediction_gpt2\n",
    "\n",
    "roberta_prediction_gpt2 = calculate_roberta_gpt2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoBERTa on GPT2-Output\n",
      "True Positive: 518 \t| True Negative: 4138\n",
      "False Positive:862 \t| False Negative:4482\n",
      "True Positive Rate:  10.36\\%\n",
      "True Negative Rate:  82.76\\%\n",
      "False Positive Rate: 17.24\\%\n",
      "False Negative Rate: 89.64\\%\n",
      "Accuracy: 46.56\\%\n",
      "F1 Score: 0.16\n",
      "LaTeX Usable-version\n",
      "\n",
      "46.56\\% &10.36\\%, (518) & 82.76\\%, (4138) & 17.24\\%, (862) & 89.64\\%, (4482) \\\\\n"
     ]
    }
   ],
   "source": [
    "roberta_statistics = quick_statistics(roberta_prediction_gpt2)\n",
    "print(\"RoBERTa on GPT2-Output\")\n",
    "report_statistics(*roberta_statistics)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate ZeroGPT\n",
    "\n",
    "### ZeroGPT on OpenGPTText-Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cache from ./data/utility_fns.py\n",
      "True Positive: 1030 \t| True Negative: 2171\n",
      "False Positive:773 \t| False Negative:1914\n",
      "True Positive Rate:  34.99\\%\n",
      "True Negative Rate:  73.74\\%\n",
      "False Positive Rate: 26.26\\%\n",
      "False Negative Rate: 65.01\\%\n",
      "Accuracy: 54.36\\%\n",
      "F1 Score: 0.43\n",
      "LaTeX Usable-version\n",
      "\n",
      "54.36\\% &34.99\\%, (1030) & 73.74\\%, (2171) & 26.26\\%, (773) & 65.01\\%, (1914) \\\\\n"
     ]
    }
   ],
   "source": [
    "zero_prediction_final = load_cache(Path(PATH_CACHE, \"eval_zerogpt_opengpt_final.pt\"))\n",
    "report_statistics(*quick_statistics(zero_prediction_final))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ZeroGPT on OpenGPTText-Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cache from ./data/utility_fns.py\n",
      "True Positive: 844 \t| True Negative: 2459\n",
      "False Positive:485 \t| False Negative:2100\n",
      "True Positive Rate:  28.67\\%\n",
      "True Negative Rate:  83.53\\%\n",
      "False Positive Rate: 16.47\\%\n",
      "False Negative Rate: 71.33\\%\n",
      "Accuracy: 56.1\\%\n",
      "F1 Score: 0.4\n",
      "LaTeX Usable-version\n",
      "\n",
      "56.1\\% &28.67\\%, (844) & 83.53\\%, (2459) & 16.47\\%, (485) & 71.33\\%, (2100) \\\\\n"
     ]
    }
   ],
   "source": [
    "zero_prediction_original = load_cache(Path(PATH_CACHE, \"eval_zerogpt_opengpt_original.pt\"))\n",
    "report_statistics(*quick_statistics(zero_prediction_original))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ZeroGPT on GPT2-Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cache from ./data/utility_fns.py\n",
      "True Positive: 476 \t| True Negative: 3832\n",
      "False Positive:1168 \t| False Negative:4522\n",
      "True Positive Rate:  9.52\\%\n",
      "True Negative Rate:  76.64\\%\n",
      "False Positive Rate: 23.36\\%\n",
      "False Negative Rate: 90.48\\%\n",
      "Accuracy: 43.09\\%\n",
      "F1 Score: 0.14\n",
      "LaTeX Usable-version\n",
      "\n",
      "43.09\\% &9.52\\%, (476) & 76.64\\%, (3832) & 23.36\\%, (1168) & 90.48\\%, (4522) \\\\\n"
     ]
    }
   ],
   "source": [
    "zero_prediction_gpt2 = load_cache(Path(PATH_CACHE, \"eval_zerogpt_gpt2_output.pt\"))\n",
    "report_statistics(*quick_statistics(zero_prediction_gpt2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate OpenAI Detector\n",
    "\n",
    "### OpenAI on OpenGPTText-Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cache from ./data/utility_fns.py\n",
      "True Positive: 596 \t| True Negative: 2800\n",
      "False Positive:144 \t| False Negative:2348\n",
      "True Positive Rate:  20.24\\%\n",
      "True Negative Rate:  95.11\\%\n",
      "False Positive Rate: 4.89\\%\n",
      "False Negative Rate: 79.76\\%\n",
      "Accuracy: 57.68\\%\n",
      "F1 Score: 0.32\n",
      "LaTeX Usable-version\n",
      "\n",
      "57.68\\% &20.24\\%, (596) & 95.11\\%, (2800) & 4.89\\%, (144) & 79.76\\%, (2348) \\\\\n"
     ]
    }
   ],
   "source": [
    "openai_prediction_final = load_cache(Path(PATH_CACHE, \"eval_openai_opengpt_final.pt\"))\n",
    "report_statistics(*quick_statistics(openai_prediction_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cache from ./data/utility_fns.py\n",
      "True Positive: 437 \t| True Negative: 2898\n",
      "False Positive:46 \t| False Negative:2507\n",
      "True Positive Rate:  14.84\\%\n",
      "True Negative Rate:  98.44\\%\n",
      "False Positive Rate: 1.56\\%\n",
      "False Negative Rate: 85.16\\%\n",
      "Accuracy: 56.64\\%\n",
      "F1 Score: 0.26\n",
      "LaTeX Usable-version\n",
      "\n",
      "56.64\\% &14.84\\%, (437) & 98.44\\%, (2898) & 1.56\\%, (46) & 85.16\\%, (2507) \\\\\n"
     ]
    }
   ],
   "source": [
    "openai_prediction_original = load_cache(Path(PATH_CACHE, \"eval_openai_opengpt_original.pt\"))\n",
    "report_statistics(*quick_statistics(openai_prediction_original))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cache from ./data/utility_fns.py\n",
      "True Positive: 2801 \t| True Negative: 4321\n",
      "False Positive:679 \t| False Negative:2199\n",
      "True Positive Rate:  56.02\\%\n",
      "True Negative Rate:  86.42\\%\n",
      "False Positive Rate: 13.58\\%\n",
      "False Negative Rate: 43.98\\%\n",
      "Accuracy: 71.22\\%\n",
      "F1 Score: 0.66\n",
      "LaTeX Usable-version\n",
      "\n",
      "71.22\\% &56.02\\%, (2801) & 86.42\\%, (4321) & 13.58\\%, (679) & 43.98\\%, (2199) \\\\\n"
     ]
    }
   ],
   "source": [
    "openai_prediction_gpt2 = load_cache(Path(PATH_CACHE, \"eval_openai_gpt2_output.pt\"))\n",
    "report_statistics(*quick_statistics(openai_prediction_gpt2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
