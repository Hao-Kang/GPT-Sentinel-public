{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Baseline Model (GPT2-Detector)\n",
    "\n",
    "## Note\n",
    "\n",
    "**Note**: According to https://github.com/openai/gpt-2-output-dataset/issues/28, it is required to use\n",
    "* `transformers == 2.9.1`\n",
    "* `tokenizers == 0.7.0`\n",
    "\n",
    "> WARNING: to install these two specific version, you MUST have your python as version of 3.8\n",
    ">\n",
    "> Because\n",
    "> 1. pip does not have pre-compiled wheel for tokenizers v0.7.0 for python 3.9+\n",
    "> 2. to compile tokenizer, you need to download rust compiler\n",
    "> 3. tokenizer 0.7.0's Rust source code is using deprecated feature in Rust language, which is NOT supported (error, not warning) by latest rust compiler\n",
    "> 4. So you can't install tokenizer 0.7.0 on the environment if you are not using python 3.8\n",
    "\n",
    "\n",
    "to make things work. Installing later versions of transformer leads to error in loading weight file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from Model import SentinelNonLinear, Sentinel, T5Sentinel\n",
    "from Dataset import OpenGPTDataset, GPT2_OutputDataset, download_gpt2\n",
    "from memoizer import memoize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2 output dataset already downloaded.\n"
     ]
    }
   ],
   "source": [
    "PATH_ROOT = Path(Path.cwd().parent.parent.parent)\n",
    "PATH_WEBTEXT = Path(PATH_ROOT, \"data\", \"open-web-text-split\")\n",
    "PATH_GPTTEXT = Path(PATH_ROOT, \"data\", \"open-gpt-text-split\")\n",
    "PATH_GPT2_OUTPUT = Path(PATH_ROOT, \"data\", \"gpt2-output\")\n",
    "\n",
    "PATH_CACHE = Path(PATH_ROOT, \"result\", \"cache\")\n",
    "\n",
    "PATH_GPT2_CHECKPT = Path(PATH_CACHE, \"detector-base.pt\")\n",
    "\n",
    "SELF_NAME = \"evaluate_gpt2_detector.ipynb\"\n",
    "\n",
    "if len([_ for _ in PATH_GPT2_OUTPUT.iterdir()]) == 0: download_gpt2(PATH_GPT2_OUTPUT)\n",
    "else: print(\"GPT2 output dataset already downloaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Human text: 100%|███████████████████████████████████| 5000/5000 [00:00<00:00, 127720.14it/s]\n",
      "Loading GPT text: 100%|█████████████████████████████████████| 5000/5000 [00:00<00:00, 123840.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<All data loaded>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "clean_opengpt_test = OpenGPTDataset(\n",
    "    str(Path(PATH_WEBTEXT, \"test.jsonl\")),\n",
    "    str(Path(PATH_GPTTEXT, \"test.jsonl\"))\n",
    ")\n",
    "\n",
    "dirty_opengpt_test = OpenGPTDataset(\n",
    "    str(Path(PATH_WEBTEXT, \"test-dirty.jsonl\")),\n",
    "    str(Path(PATH_GPTTEXT, \"test-dirty.jsonl\"))\n",
    ")\n",
    "\n",
    "gpt2xl_output_test = GPT2_OutputDataset(\n",
    "    Path(PATH_GPT2_OUTPUT, \"xl-1542M.test.jsonl\"),\n",
    "    Path(PATH_GPT2_OUTPUT, \"webtext.test.jsonl\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_statistics(prediction):\n",
    "    TP, TN, FP, FN = 0, 0, 0, 0\n",
    "    key:str\n",
    "    for key in prediction:\n",
    "        pred = prediction[key]\n",
    "        p_gpt, p_web = pred[0], pred[1]\n",
    "        pred_gpt = p_gpt > p_web\n",
    "        real_gpt = key.endswith(\"gpt\")\n",
    "\n",
    "        if pred_gpt and real_gpt: TP += 1\n",
    "        elif (not pred_gpt) and (not real_gpt): TN += 1\n",
    "        elif pred_gpt and (not real_gpt): FP += 1\n",
    "        else: FN += 1\n",
    "    \n",
    "    return TP, TN, FP, FN\n",
    "\n",
    "def report_statistics(TP, TN, FP, FN):\n",
    "    TPR = TP / (TP + FN)\n",
    "    TNR = TN / (TN + FP)\n",
    "    FPR = FP / (FP + TN)\n",
    "    FNR = FN / (FN + TP)\n",
    "    print(f\"True Positive: {TP} \\t| True Negative: {TN}\")\n",
    "    print(f\"False Positive:{FP} \\t| False Negative:{FN}\")\n",
    "    print(f\"True Positive Rate:  {round(TPR * 100, 2)}\\%\")\n",
    "    print(f\"True Negative Rate:  {round(TNR * 100, 2)}\\%\")\n",
    "    print(f\"False Positive Rate: {round(FPR * 100, 2)}\\%\")\n",
    "    print(f\"False Negative Rate: {round(FNR * 100, 2)}\\%\")\n",
    "    print(f\"Accuracy: {round(((TP + TN) / (TP + TN + FP + FN)) * 100, 2)}\\%\")\n",
    "    print(f\"F1 Score: {round((TP) / (TP + 0.5 * (FP + FN)), 2)}\")\n",
    "\n",
    "    print(\"LaTeX Usable-version\\n\\n\")\n",
    "\n",
    "    print(\n",
    "    f\"{round(((TP + TN) / (TP + TN + FP + FN)) * 100, 2)}\\%\", \"&\"\n",
    "    f\"{round(TPR * 100, 2)}\\%, ({TP})\", \"&\",\n",
    "    f\"{round(TNR * 100, 2)}\\%, ({TN})\", \"&\",\n",
    "    f\"{round(FPR * 100, 2)}\\%, ({FP})\", \"&\",\n",
    "    f\"{round(FNR * 100, 2)}\\%, ({FN})\", \"\\\\\\\\\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(text, model, tokenizer):\n",
    "    # returns [P[fake], P[real]]\n",
    "    # encode\n",
    "    tokens = tokenizer.encode(text, max_length=512)\n",
    "    tokens = tokens[:tokenizer.max_len - 2]\n",
    "    tokens = torch.tensor([tokenizer.bos_token_id] + tokens + [tokenizer.eos_token_id]).unsqueeze(0)\n",
    "    mask = torch.ones_like(tokens)\n",
    "\n",
    "    # forward propagation\n",
    "    with torch.inference_mode():\n",
    "        logits = model(tokens.to(\"cuda\"), attention_mask=mask.to(\"cuda\"))[0]\n",
    "        probs = logits.softmax(dim=-1)\n",
    "    \n",
    "    # update statistics\n",
    "    pred = probs.detach().cpu().flatten().numpy()\n",
    "    return pred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate GPT2-Output\n",
    "\n",
    "### GPT2-Detector on OpenGPTText-Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load(PATH_GPT2_CHECKPT)\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\")\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\", truncation=True, max_length=510)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model = model.to(\"cuda\")\n",
    "print(\"Model Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reusing existing cache from /mnt/d/Projects/GPT-Sentinel/result/cache/eval_gpt2_opengpt_final.pt\n",
      "Cache is generated by evaluate_gpt2_detector.ipynb\n"
     ]
    }
   ],
   "source": [
    "@memoize(Path(PATH_CACHE, \"eval_gpt2_opengpt_final.pt\"), SELF_NAME)\n",
    "def calculate_clean():\n",
    "    prediction_clean = dict()\n",
    "    for index in tqdm(range(len(clean_opengpt_test))):\n",
    "        (text, label), (uid, _) = clean_opengpt_test[index]\n",
    "        prediction = inference(text, model, tokenizer)\n",
    "        uid = uid + (\"-web\" if label == 0 else \"-gpt\")\n",
    "        prediction_clean[uid] = prediction\n",
    "    return prediction_clean\n",
    "\n",
    "prediction_clean = calculate_clean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2-Detector on OpenGPTText-Final\n",
      "True Positive: 389 \t| True Negative: 1233\n",
      "False Positive:36 \t| False Negative:2555\n",
      "True Positive Rate:  13.21\\%\n",
      "True Negative Rate:  97.16\\%\n",
      "False Positive Rate: 2.84\\%\n",
      "False Negative Rate: 86.79\\%\n",
      "Accuracy: 38.5\\%\n",
      "F1 Score: 0.23\n",
      "LaTeX Usable-version\n",
      "\n",
      "\n",
      "38.5\\% &13.21\\%, (389) & 97.16\\%, (1233) & 2.84\\%, (36) & 86.79\\%, (2555) \\\\\n"
     ]
    }
   ],
   "source": [
    "print(\"GPT2-Detector on OpenGPTText-Final\")\n",
    "report_statistics(*quick_statistics(prediction_clean))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT2-Detector on OpenGPTText-Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reusing existing cache from /mnt/d/Projects/GPT-Sentinel/result/cache/eval_gpt2_opengpt_original.pt\n",
      "Cache is generated by evaluate_gpt2_detector.ipynb\n"
     ]
    }
   ],
   "source": [
    "@memoize(Path(PATH_CACHE, \"eval_gpt2_opengpt_original.pt\"), SELF_NAME)\n",
    "def calculate_original():\n",
    "    prediction_orig = dict()\n",
    "    for index in tqdm(range(len(dirty_opengpt_test))):\n",
    "        (text, label), (uid, _) = dirty_opengpt_test[index]\n",
    "        prediction = inference(text, model, tokenizer)\n",
    "        uid = uid + (\"web\" if label == 0 else \"gpt\")\n",
    "        prediction_orig[uid] = prediction\n",
    "    return prediction_orig\n",
    "\n",
    "prediction_orig = calculate_original()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2-Detector on OpenGPTText-Final\n",
      "True Positive: 378 \t| True Negative: 1217\n",
      "False Positive:52 \t| False Negative:2566\n",
      "True Positive Rate:  12.84\\%\n",
      "True Negative Rate:  95.9\\%\n",
      "False Positive Rate: 4.1\\%\n",
      "False Negative Rate: 87.16\\%\n",
      "Accuracy: 37.86\\%\n",
      "F1 Score: 0.22\n",
      "LaTeX Usable-version\n",
      "\n",
      "\n",
      "37.86\\% &12.84\\%, (378) & 95.9\\%, (1217) & 4.1\\%, (52) & 87.16\\%, (2566) \\\\\n"
     ]
    }
   ],
   "source": [
    "print(\"GPT2-Detector on OpenGPTText-Final\")\n",
    "report_statistics(*quick_statistics(prediction_orig))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT2-Detector on GPT2-Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reusing existing cache from /mnt/d/Projects/GPT-Sentinel/result/cache/eval_gpt2_gpt2_output.pt\n",
      "Cache is generated by evaluate_gpt2_detector.ipynb\n"
     ]
    }
   ],
   "source": [
    "@memoize(Path(PATH_CACHE, \"eval_gpt2_gpt2_output.pt\"), SELF_NAME)\n",
    "def calculate_gpt2():\n",
    "    gpt2_prediction_gpt2 = dict()\n",
    "    for index in tqdm(range(len(gpt2xl_output_test))):\n",
    "        text, label = gpt2xl_output_test[index]\n",
    "        prediction = inference(text, model, tokenizer)\n",
    "        uid = str(index) + (\"web\" if label == 0 else \"gpt\")\n",
    "        gpt2_prediction_gpt2[uid] = prediction\n",
    "    return gpt2_prediction_gpt2\n",
    "\n",
    "gpt2_prediction_gpt2 = calculate_gpt2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2-Detector on OpenGPTText-Final\n",
      "True Positive: 4629 \t| True Negative: 4681\n",
      "False Positive:319 \t| False Negative:371\n",
      "True Positive Rate:  92.58\\%\n",
      "True Negative Rate:  93.62\\%\n",
      "False Positive Rate: 6.38\\%\n",
      "False Negative Rate: 7.42\\%\n",
      "Accuracy: 93.1\\%\n",
      "F1 Score: 0.93\n",
      "LaTeX Usable-version\n",
      "\n",
      "\n",
      "93.1\\% &92.58\\%, (4629) & 93.62\\%, (4681) & 6.38\\%, (319) & 7.42\\%, (371) \\\\\n"
     ]
    }
   ],
   "source": [
    "print(\"GPT2-Detector on OpenGPTText-Final\")\n",
    "report_statistics(*quick_statistics(gpt2_prediction_gpt2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatgpt_legacy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
